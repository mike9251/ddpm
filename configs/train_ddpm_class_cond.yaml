task_name: DDPM_ClassCond_Celeba_64

data_dir: path-to-celeba-dataset
labels_path: null
img_size: 64
batch_size: 32
num_workers: 12

lr: 3e-4
ddp: false
log_every: 1000
num_img_to_sample: 16
epochs: 100
resume_from: null
ckpt_every: 1
device: cuda

cfg_scale: 3.0
width: 1
num_classes: null
time_dim: 256
noise_steps: 1000
beta_start: 1e-4
beta_end: 2e-2

beta_ema: 0.995
start_step_ema: 10000

# Paths configuration
log_dir: ./logs
output_dir: ${hydra:runtime.output_dir} # this dir is formed below

hydra:
  # Output directory for logs, checkpoints, etc.
  run:
    dir: ${log_dir}/${task_name}/runs/${now:%Y-%m-%d}_${now:%H-%M-%S}


# python3 src/train_ddpm.py --config-nam=train_ddpm_class_cond data_dir=/workspace/data/celeba_hq_256 batch_size=32 num_workers=12 log_every=1000 num_img_to_sample=16 device=cuda labels_path=labels.csv num_classes=2 log_dir=/workspace/logs ckpt_every=10 epochs=201
# python3 src/train_ddpm.py --config-nam=train_ddpm_class_cond data_dir=/workspace/data/celeba_hq_256 batch_size=48 lr=4.5e-4  num_workers=16 log_every=1000 num_img_to_sample=16 device=cuda labels_path=labels.csv num_classes=2 log_dir=/workspace/logs ckpt_every=10 epochs=201 task_name=DDPM_ClassCond_Celeba_64_cfg_3